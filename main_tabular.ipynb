{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Conda Enviroment should be enabled before you run the next cell!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VSYQclcJutMs",
        "outputId": "00fdf1cb-64d5-444f-c70d-1f0bd5b84275"
      },
      "outputs": [],
      "source": [
        "# Install the requirements for this notebook\n",
        "%pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0oeO47OWutMs"
      },
      "outputs": [],
      "source": [
        "# Ensures local repository updates are reflected by colab\n",
        "%load_ext autoreload\n",
        "%autoreload"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4qTAYATutMs"
      },
      "source": [
        "## Imports and Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3k8YU5IjwPIg"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n",
        "                             fbeta_score, roc_auc_score, average_precision_score, confusion_matrix)\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "import xgboost as xgb\n",
        "from catboost import CatBoostClassifier\n",
        "\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "class Config:\n",
        "    def __init__(self):\n",
        "        # General configuration\n",
        "        self.seed = 239081663\n",
        "\n",
        "        # Model configuration\n",
        "        self.tabular_input_dim = None  # Will be set after loading data\n",
        "        self.num_classes = 2  # Adjust based on your target variable\n",
        "\n",
        "        # Data paths\n",
        "        self.data_dir = \"./Data\"\n",
        "        self.tabular_data_path = f\"{self.data_dir}/Cleaned Covid Data.csv\" \n",
        "        self.prompt_data_path = f\"{self.data_dir}/Cleaned Prompt Covid Data.csv\" \n",
        "\n",
        "        # Data splits\n",
        "        self.train_split = 0.7  # 70% for training\n",
        "        self.test_split = 1 - self.train_split  # 30% for testing\n",
        "        self.male_ratio = 1 \n",
        "        self.female_ratio = 1 - self.male_ratio\n",
        "\n",
        "        # Baseline models\n",
        "        self.tabular_model_type = 'xgboost'  # Options: 'mlp', 'xgboost', 'catboost'\n",
        "\n",
        "        # Other configurations\n",
        "        self.json_path = None  # We will use this later to output our logging file\n",
        "\n",
        "        self.target_col = 'COVID-19 PRESENCE'  # Adjust based on your dataset\n",
        "        self.columns_to_drop = [col for col in [\"COVID-19 PRESENCE\", \"COVID-19 SEVERITY\", \"DEATH\", \"PNEUMONIA\"] if col != self.target_col]\n",
        "\n",
        "\n",
        "        # Logging directory\n",
        "        self.log_dir = './loggedRuns'\n",
        "        os.makedirs(self.log_dir, exist_ok=True)\n",
        "\n",
        "        # Parameter grids\n",
        "        self.xgb_param_grid = {\n",
        "            'max_depth': [3, 6, 9],\n",
        "            'learning_rate': [0.01, 0.1],\n",
        "            'n_estimators': [100, 200],  # n_estimators corresponds to epochs\n",
        "            'subsample': [0.8, 1.0],\n",
        "            'colsample_bytree': [0.8, 1.0],\n",
        "            'gamma': [0, 0.1],\n",
        "        }\n",
        "\n",
        "        self.cat_param_grid = {\n",
        "            'depth': [3, 6, 9],\n",
        "            'learning_rate': [0.01, 0.1],\n",
        "            'iterations': [100, 200],  # iterations corresponds to epochs\n",
        "            'subsample': [0.8, 1.0],\n",
        "            'colsample_bylevel': [0.8, 1.0],\n",
        "            'l2_leaf_reg': [1, 3, 5],\n",
        "        }\n",
        "\n",
        "cfg = Config()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yn5-hhxwzuQ6"
      },
      "source": [
        "## Data Splitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nEaRw984wPDd",
        "outputId": "1d36728c-e100-41b8-865b-ecccd18ff60d"
      },
      "outputs": [],
      "source": [
        "def split_data_dual(df_tab, df_prompt, dataset_type=\"Original\", test_size=0.2, random_state=42):\n",
        "    \"\"\"\n",
        "    Splits the original tabular DataFrame and its associated prompt DataFrame into training and test sets.\n",
        "    The split is done on df_tab (which is assumed to have the same index as df_prompt) and then the same\n",
        "    indices are used to extract rows from df_prompt.\n",
        "\n",
        "    The test set is created to be balanced (50% male, 50% female) by performing stratified splitting on the SEX column.\n",
        "    The training set is then filtered based on the dataset_type:\n",
        "      - \"Original\": use the full training set.\n",
        "      - \"Gender-Balance\": sample equal numbers of males and females.\n",
        "      - \"Male-Only\": keep only male data points.\n",
        "      - \"Female-Only\": keep only female data points.\n",
        "      - \"Non-pregnant Females\": from females, select rows where PREGNANT == 2.\n",
        "      - \"Pregnant Females\": from females, select rows where PREGNANT == 1.\n",
        "\n",
        "    Returns:\n",
        "      A tuple of tuples:\n",
        "        ((train_tab, train_prompt), (test_tab, test_prompt))\n",
        "    \"\"\"\n",
        "    # Split df_tab by gender for stratification.\n",
        "    male_data = df_tab[df_tab['SEX'] == 2]\n",
        "    female_data = df_tab[df_tab['SEX'] == 1]\n",
        "    \n",
        "    # Determine test fraction for each gender. Since overall test_size is for the entire dataset,\n",
        "    # and each gender should contribute equally, we use test_size/0.5 for each group.\n",
        "    male_train, male_test = train_test_split(male_data, test_size=test_size/0.5, random_state=random_state)\n",
        "    female_train, female_test = train_test_split(female_data, test_size=test_size/0.5, random_state=random_state)\n",
        "    \n",
        "    # Combine the test sets to form a balanced test set.\n",
        "    test_df_tab = pd.concat([male_test, female_test])\n",
        "    \n",
        "    # Combine remaining data for training.\n",
        "    train_df_tab = pd.concat([male_train, female_train])\n",
        "    \n",
        "    # Filter the training set based on dataset_type.\n",
        "    if dataset_type == \"Original\":\n",
        "        filtered_train_tab = train_df_tab.copy()\n",
        "    elif dataset_type == \"Gender-Balance\":\n",
        "        min_count = min(len(male_train), len(female_train))\n",
        "        filtered_males = male_train.sample(n=min_count, random_state=random_state)\n",
        "        filtered_females = female_train.sample(n=min_count, random_state=random_state)\n",
        "        filtered_train_tab = pd.concat([filtered_males, filtered_females])\n",
        "    elif dataset_type == \"Male-Only\":\n",
        "        filtered_train_tab = train_df_tab[train_df_tab['SEX'] == 2]\n",
        "    elif dataset_type == \"Female-Only\":\n",
        "        filtered_train_tab = train_df_tab[train_df_tab['SEX'] == 1]\n",
        "    elif dataset_type == \"Non-pregnant Females\":\n",
        "        filtered_train_tab = train_df_tab[(train_df_tab['SEX'] == 1) & (train_df_tab['PREGNANT'] == 2)]\n",
        "    elif dataset_type == \"Pregnant Females\":\n",
        "        filtered_train_tab = train_df_tab[(train_df_tab['SEX'] == 1) & (train_df_tab['PREGNANT'] == 1)]\n",
        "    else:\n",
        "        raise ValueError(\"Invalid dataset_type provided. Choose from: 'Original', 'Gender-Balance', 'Male-Only', 'Female-Only', 'Non-pregnant Females', 'Pregnant Females'.\")\n",
        "    \n",
        "    # Get the indices for training and test sets.\n",
        "    train_idx = filtered_train_tab.index\n",
        "    test_idx = test_df_tab.index\n",
        "\n",
        "    # Select the corresponding rows from the prompt DataFrame.\n",
        "    train_df_prompt = df_prompt.loc[train_idx]\n",
        "    test_df_prompt = df_prompt.loc[test_idx]\n",
        "    \n",
        "    return (filtered_train_tab, train_df_prompt), (test_df_tab, test_df_prompt)\n",
        "\n",
        "# Example usage:\n",
        "# Assume df_tabular is your tabular data and df_prompts is the corresponding prompt dataframe.\n",
        "df_tabular = pd.read_csv(cfg.tabular_data_path)\n",
        "df_prompts = pd.read_csv(cfg.prompt_data_path)\n",
        "\n",
        "# For example, to split using the \"Pregnant Females\" subset for training:\n",
        "((train_tab, train_prompt), (test_tab, test_prompt)) = split_data_dual( df_tabular, df_prompts, dataset_type=\"Pregnant Females\", test_size=0.2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_tab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dropping non-feature columns and pulling target column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 584
        },
        "id": "27htbewvwPF4",
        "outputId": "bd2d4d33-bd3e-4d54-afa7-5e0f354e857a"
      },
      "outputs": [],
      "source": [
        "#DROP CODING COLUMNS    \n",
        "tab_train_data = train_tab.drop(columns=cfg.columns_to_drop, errors='ignore')\n",
        "tab_test_data = test_tab.drop(columns=cfg.columns_to_drop, errors='ignore')\n",
        "\n",
        "prompt_train_data = train_prompt.drop(columns=cfg.columns_to_drop, errors='ignore')\n",
        "prompt_test_data = test_prompt.drop(columns=cfg.columns_to_drop, errors='ignore')\n",
        "\n",
        "# Extract features (X) and target (y) for both datasets\n",
        "tab_x_train = tab_train_data.loc[:, 'AGE':'USMER']\n",
        "tab_y_train = tab_train_data[cfg.target_col]\n",
        "tab_y_train = tab_y_train - 1 \n",
        "\n",
        "\n",
        "tab_x_test = tab_train_data.loc[:, 'AGE':'USMER']\n",
        "tab_y_test = tab_train_data[cfg.target_col]\n",
        "tab_y_test = tab_y_test - 1 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-ue6puWxJMK",
        "outputId": "4f5eb5ca-8be5-4a55-dc38-2436b22b2011"
      },
      "outputs": [],
      "source": [
        "# Display all column names\n",
        "print(tab_train_data.columns.tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IiHZDEJr0yBc",
        "outputId": "1eeed506-778b-42ac-b875-882dfe7a7c3a"
      },
      "outputs": [],
      "source": [
        "tab_x_train, tab_y_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tab_x_test, tab_y_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2m_qs1pZz4Th"
      },
      "source": [
        "## Handling Class Imbalance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iyuy5bJVwPBH",
        "outputId": "d97f38ff-00fd-4ed7-c2d0-7289573bb50f"
      },
      "outputs": [],
      "source": [
        "# Compute class weights\n",
        "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(tab_y_train), y=tab_y_train)\n",
        "class_weights_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
        "print(\"Class Weights:\", class_weights_dict)\n",
        "\n",
        "# Display class distribution in training set\n",
        "class_counts = tab_y_train.value_counts().sort_index()\n",
        "class_distribution = class_counts / class_counts.sum()\n",
        "print(\"Class Distribution in Training Set:\")\n",
        "print(class_distribution)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kk9R0dKm0WFZ"
      },
      "source": [
        "### Cross-Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hzy3c-MAwO75"
      },
      "outputs": [],
      "source": [
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=cfg.seed)\n",
        "scoring = 'f1' if cfg.num_classes == 2 else 'f1_macro'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_L4m64cl8Bo4"
      },
      "source": [
        "## Logging Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DcFbnkiK8Dxw"
      },
      "outputs": [],
      "source": [
        "def setup_json(model_name, epochs, seed, classification_type, target_col, male_ratio, female_ratio):\n",
        "    json_filename = f\"log_{model_name}_ep_{epochs}_seed_{seed}_ct_{classification_type}_target_{target_col}_male_female_split_{male_ratio}:{female_ratio}.json\"\n",
        "    json_path = os.path.join('loggedRuns', json_filename)\n",
        "\n",
        "    # Ensure the log directory exists\n",
        "    os.makedirs(os.path.dirname(json_path), exist_ok=True)\n",
        "\n",
        "    # Initialize an empty JSON file with \"logs\" key\n",
        "    with open(json_path, 'w') as json_file:\n",
        "        json.dump({\"logs\": []}, json_file)  # Create an empty JSON object with \"logs\" key\n",
        "\n",
        "    print(f\"Metrics will be logged to {json_path}\")\n",
        "    return json_path\n",
        "\n",
        "def log_to_json(message, json_path, log_type='info'):\n",
        "    \"\"\"\n",
        "    Logs a message to a JSON file, converting NumPy data types to native Python types.\n",
        "\n",
        "    Args:\n",
        "        message (dict or str): The message or metrics to log.\n",
        "        json_path (str): Path to the JSON log file.\n",
        "        log_type (str): Type of log entry (e.g., 'info', 'metrics', 'error').\n",
        "    \"\"\"\n",
        "    # Helper function to convert NumPy types to native Python types\n",
        "    def convert_numpy(obj):\n",
        "        if isinstance(obj, dict):\n",
        "            return {k: convert_numpy(v) for k, v in obj.items()}\n",
        "        elif isinstance(obj, list):\n",
        "            return [convert_numpy(elem) for elem in obj]\n",
        "        elif isinstance(obj, np.integer):\n",
        "            return int(obj)\n",
        "        elif isinstance(obj, np.floating):\n",
        "            return float(obj)\n",
        "        elif isinstance(obj, np.bool_):\n",
        "            return bool(obj)\n",
        "        elif isinstance(obj, np.ndarray):\n",
        "            return obj.tolist()\n",
        "        else:\n",
        "            return obj\n",
        "\n",
        "    # Read existing logs\n",
        "    if os.path.exists(json_path):\n",
        "        with open(json_path, 'r') as json_file:\n",
        "            try:\n",
        "                data = json.load(json_file)\n",
        "            except json.JSONDecodeError:\n",
        "                data = {\"logs\": []}  # Initialize logs if the file is corrupted\n",
        "    else:\n",
        "        data = {\"logs\": []}  # Initialize logs if the file doesn't exist\n",
        "\n",
        "    # Convert the message to a JSON-serializable format\n",
        "    message_serializable = convert_numpy(message)\n",
        "\n",
        "    # Create log entry\n",
        "    log_entry = {\n",
        "        \"timestamp\": datetime.now().isoformat(),  # Add a timestamp to the log entry\n",
        "        \"type\": log_type,\n",
        "        \"message\": message_serializable\n",
        "    }\n",
        "\n",
        "    # Append new log message\n",
        "    data[\"logs\"].append(log_entry)\n",
        "\n",
        "    # Write back to JSON\n",
        "    with open(json_path, 'w') as json_file:\n",
        "        json.dump(data, json_file, indent=4)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fm1nR1IG8LEg"
      },
      "source": [
        "## Model Training and Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Od35yR8k8Rim"
      },
      "source": [
        "### XGBoost Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m3pjiERHwOoG",
        "outputId": "27ffb06f-640d-480a-adc3-739ee9170eb8"
      },
      "outputs": [],
      "source": [
        "# Map n_estimators to epochs for logging\n",
        "epochs = cfg.xgb_param_grid['n_estimators']\n",
        "\n",
        "# Set up logging for XGBoost using config values\n",
        "cfg.json_path = setup_json(\n",
        "    model_name='xgboost',\n",
        "    epochs=epochs,\n",
        "    seed=cfg.seed,\n",
        "    classification_type='binary' if cfg.num_classes == 2 else 'multi',\n",
        "    target_col=cfg.target_col,\n",
        "    male_ratio=cfg.male_ratio,\n",
        "    female_ratio=cfg.female_ratio\n",
        ")\n",
        "\n",
        "print(\"Starting XGBoost training...\")\n",
        "\n",
        "# Create an instance of the XGBClassifier\n",
        "xgb_model = xgb.XGBClassifier(random_state=cfg.seed, use_label_encoder=False, eval_metric='logloss')\n",
        "\n",
        "# Use GridSearchCV for hyperparameter tuning\n",
        "grid_search_xgb = GridSearchCV(\n",
        "    estimator=xgb_model,\n",
        "    param_grid=cfg.xgb_param_grid,\n",
        "    scoring=scoring,\n",
        "    cv=cv,\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "grid_search_xgb.fit(tab_x_train, tab_y_train)\n",
        "\n",
        "best_xgb_model = grid_search_xgb.best_estimator_\n",
        "\n",
        "print(\"XGBoost training completed.\")\n",
        "print(f\"Best XGBoost parameters: {grid_search_xgb.best_params_}\")\n",
        "print(f\"Best XGBoost {scoring}: {grid_search_xgb.best_score_:.4f}\")\n",
        "\n",
        "# Log the best parameters and score\n",
        "log_to_json(f\"Best XGBoost parameters: {grid_search_xgb.best_params_}\", cfg.json_path, 'info')\n",
        "log_to_json(f\"Best XGBoost {scoring}: {grid_search_xgb.best_score_:.4f}\", cfg.json_path, 'info')\n",
        "\n",
        "# Save the model\n",
        "xgb_model_path = os.path.join(cfg.log_dir, f\"xgb_model_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\")\n",
        "best_xgb_model.save_model(xgb_model_path)\n",
        "log_to_json(f\"XGBoost model saved to {xgb_model_path}\", cfg.json_path, 'info')\n",
        "print(f\"XGBoost model saved to {xgb_model_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "waT2iFat8-Jt"
      },
      "source": [
        "### CatBoost Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9tBLmSjj8JXI",
        "outputId": "504f68e7-b0be-4f73-e355-b5c8a87be94f"
      },
      "outputs": [],
      "source": [
        "# Map iterations to epochs for logging\n",
        "epochs = f\"{cfg.cat_param_grid['iterations'][0]}-{cfg.cat_param_grid['iterations'][-1]}\"\n",
        "\n",
        "# Set up logging for CatBoost using config values\n",
        "cfg.json_path = setup_json(\n",
        "    model_name='catboost',\n",
        "    epochs=epochs,\n",
        "    seed=cfg.seed,  # Use seed from the config\n",
        "    classification_type='binary' if cfg.num_classes == 2 else 'multi',\n",
        "    target_col=cfg.target_col,  # Use target column from the config\n",
        "    male_ratio=cfg.male_ratio, \n",
        "    female_ratio=cfg.female_ratio\n",
        ")\n",
        "\n",
        "# Initialize CatBoost model\n",
        "cat_model = CatBoostClassifier(\n",
        "    loss_function='Logloss' if cfg.num_classes == 2 else 'MultiClass',\n",
        "    eval_metric='F1' if cfg.num_classes == 2 else 'TotalF1',\n",
        "    class_weights=class_weights.tolist(),\n",
        "    random_seed=cfg.seed,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Set up GridSearchCV for CatBoost using parameter grid from config\n",
        "grid_search_cat = GridSearchCV(\n",
        "    estimator=cat_model,\n",
        "    param_grid=cfg.cat_param_grid,\n",
        "    scoring=scoring,\n",
        "    cv=cv,\n",
        "    verbose=1,\n",
        "    n_jobs=-1,\n",
        "    return_train_score=True  # Enable capturing training scores\n",
        ")\n",
        "\n",
        "# Fit GridSearchCV\n",
        "print(\"Starting CatBoost training...\")\n",
        "log_to_json(\"Starting CatBoost training...\", cfg.json_path)\n",
        "grid_search_cat.fit(tab_x_train, tab_y_train)\n",
        "print(\"CatBoost training completed.\")\n",
        "log_to_json(\"CatBoost training completed.\", cfg.json_path)\n",
        "\n",
        "# Best parameters and model\n",
        "best_cat_model = grid_search_cat.best_estimator_\n",
        "best_params = grid_search_cat.best_params_\n",
        "best_score = grid_search_cat.best_score_\n",
        "print(f\"Best CatBoost parameters: {best_params}\")\n",
        "print(f\"Best CatBoost {scoring}: {best_score:.4f}\")\n",
        "log_to_json(f\"Best CatBoost parameters: {best_params}\", cfg.json_path)\n",
        "log_to_json(f\"Best CatBoost {scoring}: {best_score:.4f}\", cfg.json_path)\n",
        "\n",
        "# Save the CatBoost model\n",
        "cat_model_path = os.path.join(cfg.log_dir, f\"cat_model_{datetime.now().strftime('%Y%m%d_%H%M%S')}.cbm\")\n",
        "best_cat_model.save_model(cat_model_path)\n",
        "log_to_json(f\"CatBoost model saved to {cat_model_path}\", cfg.json_path)\n",
        "print(f\"CatBoost model saved to {cat_model_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LgGnIe19C2V"
      },
      "source": [
        "## Evaluation Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cJiNo8btWtLg"
      },
      "outputs": [],
      "source": [
        "def specificity_score(tn, fp):\n",
        "    return tn / (tn + fp) if (tn + fp) > 0 else 0\n",
        "\n",
        "def safe_roc_auc_score(y_true, y_pred, average=None, multi_class=None):\n",
        "    # Check if there is more than one class in y_true\n",
        "    if len(set(y_true)) == 1:\n",
        "        print(\"Only one class present in y_true. ROC AUC score is not defined.\")\n",
        "        return 0  # Or return a custom value like 0.5 if you prefer\n",
        "    else:\n",
        "        if average:\n",
        "            return roc_auc_score(y_true, y_pred, average=average, multi_class=multi_class)\n",
        "        else:\n",
        "            return roc_auc_score(y_true, y_pred)\n",
        "\n",
        "\n",
        "def compute_specificity_macro_weighted(metrics, y_true, num_classes, average='macro'):\n",
        "    specificity_per_class = []\n",
        "    support = []\n",
        "\n",
        "    for i in range(num_classes):\n",
        "        tn = metrics[f'class_{i}_tn']\n",
        "        fp = metrics[f'class_{i}_fp']\n",
        "        specificity = specificity_score(tn, fp)\n",
        "        specificity_per_class.append(specificity)\n",
        "\n",
        "        # Support is the number of true instances for the class\n",
        "        support.append(np.sum(y_true == i))\n",
        "\n",
        "    if average == 'macro':\n",
        "        # Macro average: simple mean of the per-class specificity scores\n",
        "        return np.mean(specificity_per_class)\n",
        "    elif average == 'weighted':\n",
        "        # Weighted average: weighted mean, weighting by the support of each class\n",
        "        return np.average(specificity_per_class, weights=support)\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported average type. Use 'macro' or 'weighted'.\")\n",
        "\n",
        "def compute_metrics(y_true, y_pred, y_prob, average='binary', num_classes=2):\n",
        "    try:\n",
        "        # Ensure y_true and y_pred are flattened\n",
        "        y_true = np.concatenate(y_true).ravel()  # Flatten y_true\n",
        "        y_pred = np.concatenate(y_pred).ravel()  # Flatten y_pred\n",
        "    except:\n",
        "        y_true = np.array(y_true)\n",
        "        y_pred = np.array(y_pred)\n",
        "\n",
        "    metrics = {}\n",
        "    # Compute overall specificity (macro or weighted)\n",
        "    if num_classes > 2:\n",
        "        # Calculate TP, FP, TN, FN for each class in the multi-class setting\n",
        "        for i in range(num_classes):\n",
        "            metrics[f'class_{i}_tp'] = np.sum((y_true == i) & (y_pred == i))\n",
        "            metrics[f'class_{i}_fp'] = np.sum((y_true != i) & (y_pred == i))\n",
        "            metrics[f'class_{i}_fn'] = np.sum((y_true == i) & (y_pred != i))\n",
        "            metrics[f'class_{i}_tn'] = np.sum((y_true != i) & (y_pred != i))\n",
        "\n",
        "        metrics['specificity_macro'] = compute_specificity_macro_weighted(metrics, y_true, num_classes, average='macro')\n",
        "        metrics['specificity_weighted'] = compute_specificity_macro_weighted(metrics, y_true, num_classes, average='weighted')\n",
        "    else:  # Binary case\n",
        "        tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
        "        metrics['tp'] = tp\n",
        "        metrics['tn'] = tn\n",
        "        metrics['fp'] = fp\n",
        "        metrics['fn'] = fn\n",
        "        metrics['specificity'] = specificity_score(tn=metrics['tn'], fp=metrics['fp'])\n",
        "\n",
        "    # General metrics for both binary and multi-class cases\n",
        "    metrics['accuracy'] = accuracy_score(y_true, y_pred)\n",
        "    metrics['precision'] = precision_score(y_true, y_pred, average=average)\n",
        "    metrics['recall'] = recall_score(y_true, y_pred, average=average)\n",
        "    metrics['f1'] = f1_score(y_true, y_pred, average=average)\n",
        "    metrics['f2'] = fbeta_score(y_true, y_pred, beta=2, average=average)\n",
        "\n",
        "    # AUC & Average Precision: Adapted for multi-class scenarios\n",
        "    if average != 'binary':  # Multi-class scenario\n",
        "        metrics['roc_auc'] = safe_roc_auc_score(y_true, y_prob, average=average, multi_class='ovr')\n",
        "        metrics['au_prc'] = average_precision_score(y_true, y_prob, average=average)\n",
        "    else:  # Binary case\n",
        "        # y_prob = np.concatenate(y_prob).ravel()\n",
        "        y_prob = y_prob[:, 1]\n",
        "        metrics['roc_auc'] = safe_roc_auc_score(y_true, y_prob)\n",
        "        metrics['au_prc'] = average_precision_score(y_true, y_prob)\n",
        "\n",
        "    # Sensitivity is the same as recall in both cases\n",
        "    metrics['sensitivity'] = metrics['recall']\n",
        "\n",
        "    return metrics\n",
        "\n",
        "def print_metrics(metrics, num_classes):\n",
        "    print(\"Metrics Summary:\")\n",
        "    print(\"---------------------------------------------------\")\n",
        "    if num_classes > 2:\n",
        "        # Calculate TP, FP, TN, FN for each class in the multi-class setting\n",
        "        for i in range(num_classes):\n",
        "            print(f\"True Positives (TP) for class {i}:      {metrics[f'class_{i}_tp']}\")\n",
        "            print(f\"True Negatives (TN) for class {i}:      {metrics[f'class_{i}_tn']}\")\n",
        "            print(f\"False Positives (FP) for class {i}:     {metrics[f'class_{i}_fp']}\")\n",
        "            print(f\"False Negatives (FN) for class {i}:     {metrics[f'class_{i}_fn']}\")\n",
        "        print(f\"Macro Specificity:                      {metrics['specificity_macro']:.4f}\")\n",
        "        print(f\"Weighted Specificity:                   {metrics['specificity_weighted']:.4f}\")\n",
        "    else:\n",
        "        print(f\"True Positives (TP):     {metrics['tp']}\")\n",
        "        print(f\"True Negatives (TN):     {metrics['tn']}\")\n",
        "        print(f\"False Positives (FP):    {metrics['fp']}\")\n",
        "        print(f\"False Negatives (FN):    {metrics['fn']}\")\n",
        "        print(f\"Specificity:             {metrics['specificity']:.4f}\")\n",
        "\n",
        "    print(f\"Accuracy:                {metrics['accuracy']:.4f}\")\n",
        "    print(f\"Precision:               {metrics['precision']:.4f}\")\n",
        "    print(f\"Recall:                  {metrics['recall']:.4f}\")\n",
        "    print(f\"F1 Score:                {metrics['f1']:.4f}\")\n",
        "    print(f\"F2 Score:                {metrics['f2']:.4f}\")\n",
        "    print(f\"ROC AUC:                 {metrics['roc_auc']:.4f}\")\n",
        "    print(f\"Area Under PR Curve:     {metrics['au_prc']:.4f}\")\n",
        "    print(f\"Sensitivity:             {metrics['sensitivity']:.4f}\")\n",
        "    print(\"---------------------------------------------------\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J_NzshgfYWzx"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, X, y, model_name, dataset_name, cfg):\n",
        "    y_pred = model.predict(X)\n",
        "    if cfg.num_classes == 2:\n",
        "        y_pred_prob = model.predict_proba(X)  # Shape (n_samples, 2)\n",
        "    else:\n",
        "        y_pred_prob = model.predict_proba(X)\n",
        "\n",
        "    # Compute metrics using compute_metrics function\n",
        "    average = 'binary' if cfg.num_classes == 2 else 'macro'\n",
        "    metrics = compute_metrics(y, y_pred, y_pred_prob, average=average, num_classes=cfg.num_classes)\n",
        "\n",
        "    # Print metrics using print_metrics function\n",
        "    print(f\"--- {model_name} Metrics on {dataset_name} Set ---\")\n",
        "    print_metrics(metrics, cfg.num_classes)\n",
        "\n",
        "    # Plot confusion matrix\n",
        "    cm = confusion_matrix(y, y_pred)\n",
        "    metrics['confusion_matrix'] = cm.tolist()  # Ensure it's serializable\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
        "                xticklabels=np.unique(y),\n",
        "                yticklabels=np.unique(y))\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('Actual')\n",
        "    plt.title(f'{model_name} Confusion Matrix ({dataset_name} Set)')\n",
        "    plt.show()\n",
        "\n",
        "    # Add dataset and model name to metrics\n",
        "    metrics.update({\n",
        "        'dataset': dataset_name,\n",
        "        'model_name': model_name,\n",
        "    })\n",
        "    return metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhxHzdVM9Lkg"
      },
      "source": [
        "### XGBoost Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "jRFtOxpo8JSw",
        "outputId": "683db1a3-1192-46f5-8b8b-09f15af34196"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model on different datasets\n",
        "metrics_train_xgb = evaluate_model(best_xgb_model, tab_x_train, tab_y_train, model_name='XGBoost',dataset_name='Training', cfg=cfg)\n",
        "metrics_test_xgb = evaluate_model(best_xgb_model, tab_x_test, tab_y_test, model_name='XGBoost', dataset_name='Test', cfg=cfg)\n",
        "\n",
        "# Log metrics\n",
        "log_to_json(metrics_train_xgb, cfg.json_path, 'metrics')\n",
        "log_to_json(metrics_test_xgb, cfg.json_path, 'metrics')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSoMChTX9O7_"
      },
      "source": [
        "### CatBoost Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "UBvSlCi_9N2A",
        "outputId": "fd226f26-4b81-479c-9503-be5df929ff07"
      },
      "outputs": [],
      "source": [
        "# Evaluate CatBoost model on training set\n",
        "metrics_train_cat = evaluate_model(best_cat_model, tab_x_train, tab_y_train, model_name='CatBoost', dataset_name='Training', cfg=cfg)\n",
        "\n",
        "\n",
        "# Evaluate CatBoost model on test set\n",
        "metrics_test_cat = evaluate_model(best_cat_model, tab_x_test, tab_y_test, model_name='CatBoost', dataset_name='Test', cfg=cfg)\n",
        "\n",
        "# Log metrics\n",
        "log_to_json(metrics_train_cat, cfg.json_path, 'metrics')\n",
        "log_to_json(metrics_test_cat, cfg.json_path, 'metrics')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0RGhjZBF9TiA"
      },
      "source": [
        "## Feature Importance and Plots"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5W0udVzO9ZHr"
      },
      "source": [
        "### XGBoost Feature Importance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "id": "TplQ_f7e8JQo",
        "outputId": "85b1d3b7-7c96-4747-b140-13920ef4bd22"
      },
      "outputs": [],
      "source": [
        "# Compute and log feature importance\n",
        "num_top_features = 20\n",
        "\n",
        "# Feature importance\n",
        "importance = best_xgb_model.get_booster().get_score(importance_type='weight')\n",
        "\n",
        "importance_df = pd.DataFrame({\n",
        "    'Feature': list(importance.keys()),\n",
        "    'Importance': list(importance.values())\n",
        "})\n",
        "\n",
        "# Ensure all features from the dataset are included\n",
        "all_features = list(tab_x_train.columns)  # Get all feature names from the dataset\n",
        "\n",
        "# Create a DataFrame with all features, setting missing ones to 0 importance\n",
        "full_importance_df = pd.DataFrame({'Feature': all_features})\n",
        "\n",
        "# Merge with existing importance data, filling NaN values with 0\n",
        "full_importance_df = full_importance_df.merge(importance_df, on='Feature', how='left').fillna(0)\n",
        "\n",
        "full_importance_df['Importance'] /= full_importance_df['Importance'].sum()\n",
        "full_importance_df = full_importance_df.sort_values(by='Importance', ascending=False)[:num_top_features]\n",
        "\n",
        "# Plot the feature importance\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(full_importance_df['Feature'], full_importance_df['Importance'])\n",
        "plt.title('XGBoost Feature Importance')\n",
        "plt.xlabel('Importance')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.show()\n",
        "\n",
        "# Log feature importance\n",
        "feature_importance_dict = full_importance_df.to_dict('records')\n",
        "log_to_json({'feature_importance': feature_importance_dict}, cfg.json_path, 'feature_importance')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0h5_YTTP9bxb"
      },
      "source": [
        "### CatBoost Feature Importance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "WYsGIkT_8JOb",
        "outputId": "e6418968-b707-43b4-9650-7bb0d2e1b790"
      },
      "outputs": [],
      "source": [
        "# Feature importance\n",
        "importance = best_cat_model.get_feature_importance()\n",
        "# Ensure that the length of importance matches the number of features\n",
        "if len(importance) != tab_x_train.shape[1]:\n",
        "    raise ValueError(\"Mismatch between number of features and feature importance scores.\")\n",
        "\n",
        "importance_df = pd.DataFrame({\n",
        "    'Feature': tab_x_train.columns,\n",
        "    'Importance': importance\n",
        "})\n",
        "importance_df['Importance'] /= importance_df['Importance'].sum()\n",
        "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Plot the feature importance\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(importance_df['Feature'], importance_df['Importance'])\n",
        "plt.title('CatBoost Feature Importance')\n",
        "plt.xlabel('Importance')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.show()\n",
        "\n",
        "# Log feature importance\n",
        "feature_importance_dict = importance_df.to_dict('records')\n",
        "log_to_json({'feature_importance': feature_importance_dict}, cfg.json_path, 'feature_importance')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "VIP",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
